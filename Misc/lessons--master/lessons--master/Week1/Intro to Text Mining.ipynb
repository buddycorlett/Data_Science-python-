{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Test](DataInfoKnowledge.png \"Title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'My name is Himanshu Aggarwal. I am the \"data analytics lead instructor at Ironhack'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['My', 'name', 'is', 'Himanshu', 'Aggarwal.', 'I', 'am', 'the', '\"data', 'analytics', 'lead', 'instructor\"', 'at', 'Ironhack']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "print(text.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "This is a more robust implementation of the split statement as I am using Regular Expression as well\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = re.sub(\"[^\\w]\", \" \",  text).split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "QUICK QUIZ\n",
    "\n",
    "Can anyone tell me what is the difference between the two implementations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    "A good resource is https://pythonspot.com/nltk-stemming/\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "words = word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['My',\n",
       " 'name',\n",
       " 'is',\n",
       " 'Himanshu',\n",
       " 'Aggarwal',\n",
       " '.',\n",
       " 'I',\n",
       " 'am',\n",
       " 'the',\n",
       " '``',\n",
       " 'data',\n",
       " 'analytics',\n",
       " 'lead',\n",
       " 'instructor',\n",
       " \"''\",\n",
       " 'at',\n",
       " 'Ironhack']"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing Stop Words "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "This works like the split() that we have used earlier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from nltk.corpus import stopwords\n",
    "#set(stopwords.words('english'))  ## This gives us a list of all the stop words\n",
    "stops = set(stopwords.words('english'))\n",
    "## If we print the list \"STOPS\", we will see that is has all words in the smaller letters. \n",
    "## We will talk about this, its implications, in detail later "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Aren'T\", 'When', 'Again', 'Shan', 'Off', 'Were', 'Re', 'Weren', 'We', 'Isn', 'But', 'Up', \"Weren'T\", \"Shan'T\", \"Needn'T\", 'Had', \"She'S\", 'Once', 'Of', 'From', 'Themselves', 'Don', 'At', 'Other', 'Ours', 'He', 'Their', 'D', \"Wasn'T\", 'Those', 'Our', 'Between', 'Aren', \"Haven'T\", 'Wasn', \"You'Ll\", \"That'Ll\", 'Ourselves', 'So', 'Each', \"Doesn'T\", 'The', 'Whom', 'Mightn', 'As', 'Didn', 'Himself', 'Into', 'It', 'Out', 'Myself', \"Mightn'T\", 'Doesn', 'Here', \"Didn'T\", 'Did', 'Shouldn', 'Very', 'Such', 'They', 'O', \"It'S\", 'Its', 'Won', 'Same', 'M', 'Down', 'While', 'You', 'Hers', 'Do', \"You'D\", 'In', 'I', 'Will', 'Was', 'No', 'Too', 'Now', 'Where', 'Wouldn', \"Hasn'T\", 'Itself', \"Mustn'T\", 'Having', 'About', 'Being', \"You'Ve\", 'To', 'Through', 'Me', 'Have', 'By', 'More', 'During', 'Ve', 'Over', \"Wouldn'T\", 'These', 'His', \"Hadn'T\", 'Hasn', 'Who', 'After', 'T', 'Her', \"Should'Ve\", 'This', 'Most', 'Own', 'Few', \"Couldn'T\", 'Why', 'Until', \"Won'T\", 'She', 'With', 'Some', 'Him', 'Above', 'Can', 'Than', 'For', 'Ma', 'Has', 'A', \"Isn'T\", 'Then', 'Be', 'Is', 'Hadn', 'Yourself', 'Theirs', 'Ain', 'Herself', 'Needn', 'Y', 'Yours', 'Are', 'And', 'Your', 'What', 'Haven', 'If', 'That', 'Against', 'Been', \"Shouldn'T\", 'Nor', 'Because', 'Any', 'Yourselves', 'My', 'On', 'Couldn', 'Both', \"Don'T\", 'An', 'Under', 'Them', 'Before', \"You'Re\", 'Only', 'Should', 'Doing', 'Which', 'S', 'There', 'Not', 'Just', 'How', 'Ll', 'Further', 'Or', 'Below', 'Am', 'Mustn', 'Does', 'All']\n"
     ]
    }
   ],
   "source": [
    "stops2 = []\n",
    "for each in stops:\n",
    "    stops2.append(each.title())\n",
    "print(stops2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = re.sub(\"[^\\w]\", \" \",  text).split()\n",
    "wordsFiltered = []\n",
    "for w in words:\n",
    "    if w not in stops:\n",
    "        wordsFiltered.append(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['My',\n",
       " 'name',\n",
       " 'Himanshu',\n",
       " 'Aggarwal',\n",
       " 'I',\n",
       " 'data',\n",
       " 'analytics',\n",
       " 'lead',\n",
       " 'instructor',\n",
       " 'Ironhack']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordsFiltered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "As we can see, there is a letter \"I\" in the filtered words which we do not need\n",
    "Therefore to remove this we will use \"stop2\" which is the list of capitalized words of stop list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['name', 'Himanshu', 'Aggarwal', 'data', 'analytics', 'lead', 'instructor', 'Ironhack']\n"
     ]
    }
   ],
   "source": [
    "wordsFilteredFinal = []\n",
    "for w in wordsFiltered:\n",
    "    if w not in stops2:\n",
    "        wordsFilteredFinal.append(w)\n",
    "print(wordsFilteredFinal)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stemming "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name\n",
      "himanshu\n",
      "aggarw\n",
      "data\n",
      "analyt\n",
      "lead\n",
      "instructor\n",
      "ironhack\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "for word in wordsFilteredFinal:\n",
    "    print(ps.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "It is important to know that there are more stemming algorithms, but Porter (PorterStemer) is the most popular\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### POS Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/dawg/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('name', 'NN'),\n",
       " ('Himanshu', 'NNP'),\n",
       " ('Aggarwal', 'NNP'),\n",
       " ('data', 'NNS'),\n",
       " ('analytics', 'NNS'),\n",
       " ('lead', 'JJ'),\n",
       " ('instructor', 'NN'),\n",
       " ('Ironhack', 'NNP')]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "nltk.pos_tag(wordsFilteredFinal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "A complete example \n",
    "\n",
    "https://www.kaggle.com/sasikala11/sentiment-analysis-using-python\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
